\chapter{Related Work}
\label{chapter.related}
This chapter presents the related work that has been taking place on the research topics of this thesis.
Our focus is the efficient utilization of asymmetric multi-core systems.
There has been plenty of research on asymmetric multi-core systems. 
Some of the studies focus on the system design~\cite{Kumar_micro_2003,Balakrishnan:ISCA2005, Morad_area_based}, while others explore the challenges that appear in efficiently utilizing such a heterogeneous system~\cite{Kumar:ISCA2004,Joao:ASPLOS2012,Joao:ISCA2013}.
%Kumar et al~\cite{Kumar_micro_2003} present the idea of an AMC system and proposed a feedback-based way to dynamically migrate processes among the different cores. 
%To determine the core that most effectively executed a workload, Kumar et al~\cite{Kumar:ISCA2004} proposed the use of sampling. 
%This method minimizes the execution time of each single thread and increases performance. 
%Other studies focused on the pipeline design of such AMCs and the area that should be devoted to each component in the system~\cite{Balakrishnan:ISCA2005, Morad_area_based}. 
%Other works on AMCs focus on hardware support for critical section detection~\cite{Suleman:APLOS2009} or bottleneck detection~\cite{Joao:ASPLOS2012,Joao:ISCA2013}.
%Such approaches are orthogonal to the contributions of this thesis and could benefit from them to further improve the final performance of the system.


Our main contributions can be split into two categories:
i) scheduling 
ii) software-hardware co-design
%As this is a broad topic, there have been plenty of studies in the research community.
%Some studies focus on the system design, some others focus on flexible runtime software for scheduling and others on hardware-software co-design. 
We separate the research studies according to the category the fall to and explain how they relate to our contributions.

Section~\ref{sec.related.AMC} shows the work that has been done on AMCs and is mostly generic and related to either the system design of these systems or exploring the challenges that appear in efficiently utilizing such heterogeneous systems. 
%Some of the studies focus on the system design~\cite{Kumar_micro_2003,Balakrishnan:ISCA2005, Morad_area_based}, while others explore the challenges that appear in efficiently utilizing such a heterogeneous system~\cite{Kumar:ISCA2004,Joao:ASPLOS2012,Joao:ISCA2013}.

Section~\ref{sec.related.scheduling} shows the work that has been done and relates to scheduling for heterogeneous systems.
This includes process scheduling as well as task scheduling. 
The process scheduling works relate more to the work presented in Chapter~\ref{chapter.RTS}, while the works of task scheduling relate to the work presented in Chapter~\ref{chapter.scheduling}.
We extensively study the research of task-based scheduling and we categorize the scheduling heuristics found in the literature. 
Furthermore, we present some studies that focus on heterogeneous systems with compute accelerators and finally works on schedulers that assume some sort of task criticality in their decisions.

Section~\ref{sec.related.taskgenx} presents the work related to hardware-software co-design. 
This is also related to asymmetric multi-core systems and scheduling but in focuses in another type of contribution.
Works that fall in this category differ from scheduling algorithms as they include a specific-purpose hardware design as well as a runtime system that utilizes this hardware efficiently.
In our case the proposed hardware is an asymmetric system with a compute accelerator.




\newpage













\section{Asymmetric Multi-Core Systems}
\label{sec.related.AMC}
There have been plenty of studies on asymmetric multi-core systems. 
Some studies focus on the system design, while others explore the challenges that appear in efficiently utilizing such a heterogeneous system~\cite{AMC_survey}.
Kumar et al~\cite{Kumar_micro_2003} present the idea of an AMC system and proposed a feedback-based way to dynamically migrate processes among the different cores. 
Their approach led to energy savings but at the cost of performance degradation. 
To determine the core that most effectively executed a workload, Kumar et al~\cite{Kumar:ISCA2004} proposed the use of sampling. 
This method minimizes the execution time of each single thread and increases performance. 
Other studies focused on the pipeline design of such AMCs and the area that should be devoted to each component in the system~\cite{Balakrishnan:ISCA2005, Morad_area_based}. 
%Other works on AMCs focus on hardware support for critical section detection~\cite{Suleman:APLOS2009} or bottleneck detection~\cite{Joao:ASPLOS2012,Joao:ISCA2013}.
Other studies on AMCs proposed hardware support to detect critical sections of a parallel application and schedule the most critical ones on the big cores~\cite{Suleman:APLOS2009, Joao:ASPLOS2012,Joao:ISCA2013}. 
These approaches are orthogonal to the ones evaluated in this thesis and could benefit from them to further improve the final performance of the system.


\section{Scheduling for Heterogeneous Systems}
\label{sec.related.scheduling}
%\mm{The past work has predominantly considered sequential applications and has mostly been simulation-based, which limits the size of possible workloads and the ability to accurately measure performance and energy. We found that our conclusions differ as a result of these differences in experimental methods.}



\subsection{Process Scheduling}
Process scheduling on AMCs is one of the most challenging topics in this area of study.
Bias scheduling~\cite{Koufaty_bias} is an OS scheduler that characterizes the running 
threads according to their memory or execution intensity. 
It then schedules the computation intensive threads to the big cores of the system while the memory intensive threads to the little cores of the system.
The experimental evaluation is done on Intel Xeon processors and the heterogeneous system is emulated by changing the configuration of three out of the four cores of the processor.

Cong et al propose the Energy-Efficient~\cite{Cong_quickIA} OS scheduler based on energy estimation. The evaluation is performed on the Intel QuickIA~\cite{quickIA} platform that integrates an Intel Xeon with an Atom processor. 
Code instrumentation is used to schedule different phases of the application on each processor. However, the separate core and memory subsystems in these architectures incur power and performance overheads for application migration, which makes dynamic mapping ineffective for fine-grained migration

Van Craeynest et al.~\cite{VanCraeynest_fairness} propose the fairness-aware OS scheduler that focuses on AMC architectures. 
They focus on heterogeneous single-ISA architectures and they use some of the applications evaluated in this thesis but their research lacks the real heterogeneous system; instead they simulate a heterogeneous processor with big and little cores to estimate performance and fairness (not energy).

The performance impact estimation (PIE) scheduler~\cite{VanCraeynest_PIE} is based on the impact of memory-level parallelism (MLP) and instruction-level parallelism (ILP) on the overall cycles per instruction (CPI) and focuses on improving performance.
The scheduler predicts the impact of each different core-type of the system on the MLP, ILP and it assumes hardware support for CPI. 
The evaluation of this Chapter is performed through simulation by mimicking a heterogeneous system like Arm big.LITTLE. 
Further there are no energy or power measurements.


Rodrigues et al~\cite{Rodrigues_thread_scheduling} propose a thread scheduling technique that estimates power and performance when deciding to assign a thread to a specific core of the heterogeneous system. 
However their evaluation is based on simulated results and not on a real platform. %They have info for power.

Further to these existing studies there are the state-of-the-art mainstream OS schedulers that are currently used for Arm big.LITTLE systems.
These include \textit{cluster switching (CS)}~\cite{samsung}, 
\textit{in-kernel switch (IKS)}~\cite{IKS} and \textit{global task scheduling (GTS)}~\cite{samsung}.
The CS approach allows only one of the core clusters to be active at any given moment and the OS scheduler acts as if it schedules a symmetric multi-core system.
In the IKS each little core is paired with one big core; depending on the CPU utilization the system choses which core type will represent each pair of cores.
In contrast to the above, GTS allows all cores to be active any any given moment and depending on the CPU utilization schedules threads to the appropriate cores.
These approaches are described in more detail in Chapter~\ref{chapter.study} where GTS is evaluated.

Finally, Energy-Aware Scheduling (EAS) is an on-going effort in the Linux community to introduce the energy factor in the OS scheduler~\cite{EAS, EAS_Linux}. 
It is based on performance and power profiling to set performance and power capacities and let the Linux completely fair scheduler assign slots to processes considering the different core capacities. 
EAS is not yet part of the Linux kernel and, therefore, GTS is the most sophisticated state of the art scheduling method in production on current big.LITTLE processors.


\subsection{Task Scheduling}

The search for efficient task scheduling on multi-core systems has been intensively studied. 
Most scheduling heuristics in the literature target homogeneous multiprocessors, nevertheless there exist an important number of studies in heterogeneous multiprocessors. 
In this section we give an overview of different categories of schedulers for heterogeneous systems, we explain some details about schedulers targeting specific systems using compute accelerators and explain details of previous works on criticality-aware schedulers.


\subsubsection{Categories of Heterogeneous Schedulers}

There are previous works on schedulers for heterogeneous systems that form four different types of schedulers: listing, clustering, guided-random, and duplication-based schedulers.

Listing schedulers~\cite{List, DCPS, LDCP, HEFT, CrPathDup} have two scheduling stages. In the first stage, each task is given a priority based on the policy defined in each algorithm. In the second stage, tasks are assigned to processors depending on their priorities. Most criticality-aware schedulers fall in this category, and we discuss them in Section~\ref{sec.relwork_critical}. The schedulers proposed in this thesis are also  listing schedulers.

Clustering schedulers~\cite{Hypertool, DSC, DCPS, Hetero95} first separate tasks into clusters, where each cluster is to be executed on the same processor. During the clustering stage, the algorithm assumes an unlimited number of available processors in the system. If the number of clusters exceeds the number of available cores, the \textit{merging} stage joins multiple clusters so that they match the number of available processors. An example is the Levelized Min Time~\cite{Hetero95} clustering scheduler. This heuristic clusters tasks that can execute in parallel according to their \textit{level} (i.e. sibling nodes in a graph have the same level), and assigns priorities to the tasks in a cluster according to its execution time, (i.e. tasks with the highest execution time have the highest priority). The task-to processor assignment is done in decreasing order of priority.

Guided-random schedulers~\cite{Gen07, Chemical, Dyn05} randomize their schedules by applying policies influenced by other sciences. Genetic algorithms~\cite{Gen07} group tasks into generations and schedule them according to a randomized genetic technique. Chemical reaction algorithms~\cite{Chemical} mimic molecular interactions to map tasks to processors. Some of these guided-random approaches are designed for heterogeneous systems~\cite{Gen07, Chemical}. The scheduler by Page et al.~\cite{Dyn05} enables dynamic scheduling of multiple-sized tasks for heterogeneous systems. However, it does not support dependencies between tasks.

Duplication-based schedulers~\cite{Dup03, Dup11, Dup09} aim to eliminate communication costs between processors by scheduling tasks and their successors on the same processor. If a task has many successors, it is duplicated and executed in multiple cores prior to its successors so all successor tasks get the data from their predecessors with the lowest communication cost. This scheduling potentially introduces redundant duplications of tasks which may lead to bad schedules. The Heterogeneous Economical Duplication scheduler~\cite{Dup09} performs task duplication in an economical manner as it removes the redundant duplicates if they do not affect performance. 

These previous works schedule tasks statically and assume the prior knowledge of the task execution times on the different processor types in the heterogeneous system.
In addition, none of these works, take into account the criticality of tasks with respect to task dependencies.

\subsubsection{Schedulers for Compute Accelerators}

The schedulers in the previous subsection target the scheduling of generic TDGs on generic heterogeneous architectures. In this subsection we cover schedulers that target specific systems with compute accelerators. These works are more focused on the scheduling of tasks on the target platform based on the abstractions provided by the corresponding mixture of programming models for the general-purpose processors and the compute accelerators in the system.

Most heterogeneous systems with compute accelerators nowadays combine general-purpose CPUs and GPU compute accelerators. There is a set of programming models providing abstractions to ease the development of applications on these platforms. OmpSs~\cite{OmpSs_PPL11, OmpSs} offers this abstraction by allowing multiple implementations of a given task to be executed on different processing units~\cite{Judit}. The scheduler then assigns the execution of a task to the best resource according to its earliest finish time. Another case is StarPU~\cite{starpu}, a library that offers runtime heterogeneity support and provides priority schedulers for task-to-processor allocation. AHP~\cite{AHP} is another framework that generates software pipelines for heterogeneous systems and schedules tasks to their earliest executor, based on profiling information gathered prior to runtime.

None of these works, however, take into account the criticality of tasks regarding task dependencies, but they rather focus on the earliest execution time of individual tasks on the processor types in the specific system configuration.

%The above schedulers target scheduling as a generic challenge of on any heterogeneous architecture. 
%Nevertheless, there have been plenty of studies for schedulers that target specific systems with compute accelerators. 
%These works focus mostly on the abstractions provided by the corresponding mixture of programming models for the general-purpose processors and the compute accelerators in the system.
%Most heterogeneous systems with compute accelerators nowadays combine general-purpose CPUs and GPU compute accelerators. There is a set of programming models providing abstractions to ease the development of applications on these platforms. OmpSs~\cite{OmpSs_PPL11, OmpSs} offers this abstraction by allowing multiple implementations of a given task to be executed on different processing units~\cite{Judit}. The scheduler then assigns the execution of a task to the best resource according to its earliest finish time. Another case is StarPU~\cite{starpu}, a library that offers runtime heterogeneity support and provides priority schedulers for task-to-processor allocation. AHP~\cite{AHP} is another framework that generates software pipelines for heterogeneous systems and schedules tasks to their earliest executor, based on profiling information gathered prior to runtime.



\subsubsection{Criticality-Aware Schedulers}
\label{sec.relwork_critical}

Several previous works propose scheduling heuristics that focus on the critical path of a TDG to reduce total execution time~\cite{DCPS, LDCP, HEFT, CrPathDup, Moschakis2015}. To identify the tasks on the critical path, most of these works use the concept of \textit{upward rank} and \textit{downward rank}. The upward rank of a task is the maximum sum of computation and communication cost of the tasks in the dependency chains from that task to an exit node in the graph. The downward rank of a task is the maximum sum of computation and communication cost of the tasks in the dependency chain from an entry node to that task. Each task has an upward rank and downward rank for each processor type in the heterogeneous system, as the computation and communication costs differ across core types.

The Heterogeneous Earliest Finish Time (HEFT) algorithm~\cite{HEFT} maintains a list of tasks sorted in decreasing order of their upward rank. At each schedule step, HEFT assigns the task with the highest upward rank to the processor that finishes the execution of the task at the earliest possible time. Another work is the Longest Dynamic Critical Path (LDCP) algorithm~\cite{LDCP}. LDCP also statically schedules first the task with the highest upward rank on every schedule step. The difference between LDCP and HEFT is that LDCP updates the computation and communication costs on multiple processors of the scheduled task by the costs discovered in the processor to which it was assigned.

The Critical-Path-on-a-Processor (CPOP) algorithm~\cite{HEFT} also maintains a list of tasks sorted in decreasing order as in HEFT, but in this case it is ordered according to the addition of their \textit{upward rank} and \textit{downward rank}. The tasks with the highest \textit{upward rank + downward rank} belong to the critical path. On each step, these tasks are statically assigned to the processor that minimizes the critical-path execution time.

%The drawback of static listing algorithms is the static priority assignment can lead to wrong schedules at runtime~\cite{DCP}. 

The main weaknesses of these works are that (a) they assume prior knowledge of the computation and communication costs of each individual task on each processor type, (b) they operate statically on the whole TDG, so they do not apply to dynamically scheduled applications where only a part of the TDG is available at any given time, and (c) most of them use synthetic TDGs that are not necessarily representative of the dependencies in real workloads.




\section{Hardware-Software co-design}
\label{sec.related.taskgenx}
%Papers to add:
%1. Flexible Architectural Support for Fine-Grain Scheduling, Kozyrakis
%2. Emilio's papers CATA
%3. Carbon by Kumar et.al.
%4. Xubin's, Jaume's
%5. Nexus
%6. Task superscalar\cite{TaskSS}


In this thesis, apart from proposing scheduling algorithms for task-based programming models, we also provide a hardware-software proposal for accelerating the most intensive parts of the runtime system.
This section provides the related work on task-based programming models as well as on hardware-software co-design proposals like TaskGenX.

%Our approach is a new task-based runtime system design that enables the acceleration of task creation to overcome important bottlenecks in performance.
Task-based programming models are widely spread as they facilitate the parallel execution on homogeneous or heterogeneous multi-core environments.
State of the art task-based runtime systems include the OpenMP~\cite{OpenMP}, OmpSs~\cite{OmpSs_PPL11}, StarPU~\cite{starpu}, Cilk++~\cite{Cilk,Cilk++} and Swan~\cite{Vandierendonck:PACT2011}.
All these models support tasks and maintain a TDG specifying the inter-task dependencies.
This means that the runtime system is responsible for the task creation, the dependence analysis as well as the scheduling of the tasks.
A different runtime system organization was proposed by Bosch et.al~\cite{DAST,JaumeMaster}.
%The idea of a runtime system that offloads parts of the runtime was described by Bosch et.al~\cite{DAST,JaumeMaster}.
In this system, some of the runtime activities are automatically offloaded on the special DAST thread so the worker threads are able to execute tasks uninterruptedly.  
Even if there is a great amount of studies on parallel runtime systems, none of these offers automatic offloading of task creation.

The fact that task-based programming models are so widely spread makes TaskGenX-like approaches very important and also gives importance to studies that focus on adding hardware support to boost performance of task-based runtime systems.
%on how to boost performance of such programming models with hardware support.
%Many works in the research community focus on adding hardware support to boost performance of task-based runtime systems by reducing the runtime overheads.
Even if their work focuses more on the hardware part of the design, their contributions are very relative to our study as we can distinguish which parts of the hardware is more beneficial to be accelerated.

Carbon~\cite{Carbon} accelerates the scheduling of tasks by implementing hardware ready queues.
Carbon maintains one hardware queue per core and accelerates all possible scheduling overheads by using these queues.
Nexus\#~\cite{Nexus} is also a distributed hardware accelerator capable of executing the \textit{in}, \textit{out}, \textit{inout}, \textit{taskwait} and \textit{taskwait on} pragmas, namely the task dependencies.
TDM~\cite{Emilio:HPCA} mitigates runtime system overheads by introducing a distributed hardware unit, denoted Dependence Management Unit (DMU), and minimal ISA extensions that allow the runtime system to offload costly dependence tracking operations.
Unlike Carbon, TDM and Nexus, {\proposal} accelerates only task creation.
Moreover, ADM~\cite{Sanchez:2010} is another distributed approach that proposes hardware support for the inter-thread communication to avoid going through the memory hierarchy. 
This aims to provide a more flexible design as the scheduling policy can be freely implemented in software.
These designs require the implementation of a hardware component for each core of an SoC.
Our proposal assumes a centralized hardware unit that is capable of operating without the need to change the SoC.

Task Superscalar~\cite{TaskSS} and Picos++~\cite{Xubin} use a single hardware component to accelerate parts of the runtime system.
In the case of Task superscalar, all the parts of the runtime system are transferred to the accelerator.
Picos++~\cite{Xubin} is a hardware-software co-design that supports nested tasks. 
This design enables the acceleration of the inter-task dependencies on a special hardware.
Swarm~\cite{Swarm} performs speculative task execution. 
Instead of accelerating parts of the runtime system, Swarm uses hardware support to accelerate speculation.
This is different than our design that decouples only task creation.

%In our paper we present a flexible runtime system that supports the acceleration of task creation.
Our work diverges to prior studies for two main reasons that rely on their implementation and their practicability. 
First, their implementation typically requires changes in hardware of the SoC.
	This means that they need an expensive design where each core of the chip has an extra component.
	TaskGenX offers a much cheaper solution by requiring only a single specialized core that, according to our experiments, can manage the task creation for 512-core SoCs.
Secondly, none of the previous studies is aiming at accelerating exclusively task creation overheads. 
	According to our study task creation becomes the main bottleneck as we increase the number of cores and our proposal is the first that takes this into account resulting in a minimalistic approach of runtime overheads acceleration.



\if 0
Similar to OS scheduling approaches there have been many task scheduling approaches that are directed for utilizing AMCs.
The Levelized Min Time~\cite{Hetero95} heuristic first clusters the tasks that can execute in parallel (\textit{levels}) and then it assigns priorities to them, according to their execution time.
The Dynamic Level Scheduling algorithm~\cite{Hetero93} assigns the tasks to the processors according to their \textit{dynamic level} (DL).
% which is \textit{DL(n) = SL(n) - earliest exec time(n)}. 
%Random graph generators on simulated heterogeneous processors. 
Heterogeneous Economical Duplication (HED)~\cite{Dup09} duplicates the tasks in order to be executed on more than one cores but it then
%performs task duplication in an economical manner as it 
removes the redundant duplicates if they do not affect the makespan. 
%
% our contribution of the thesis: !!!
%CATS scheduler~\cite{Chronaki:ICS2015} is designed for AMCs like big.LITTLE and dynamically schedules the \textit{critical} tasks to the big cores of the system to increase performance. 
%
Topcuoglu et al proposed the Heterogeneous Earliest Finish Time (HEFT) scheduler that statically assigns each task to the processor that will finish it at the earliest possible time. To do so, it keeps records with the task costs for each processor type.
%The Heterogeneous Earliest Finish Time (HEFT) algorithm \cite{HEFT} maintains a list of tasks sorted in decreasing order of their \textit{upward rank}. At each schedule step, HEFT assigns the task with the highest \textit{upward rank} to the processor that finishes the execution of the task at the earliest possible time. Lack of desktop applications and no mention of real heterogeneous system.
They also proposed the Critical Path on a Processor (CPOP) algorithm \cite{HEFT} that maintains a list of tasks and statically identifies and schedules the tasks belonging to the critical path  to the processor that minimizes the sum of their execution times. 
%Critical Path on a Processor (CPOP) algorithm \cite{HEFT} also maintains a list of tasks but, in this case, tasks are sorted decreasingly according to their \textit{upward rank + downward rank}. 
%The tasks with the highest \textit{upward rank + downward rank} are the tasks that belong to the critical path and these are sent to the \textit{critical path processor} that is the one that minimizes the sum of their execution times. Lack of desktop applications and no mention of real heterogeneous system.
The Longest Dynamic Critical Path (LDCP) algorithm \cite{LDCP} identifies the tasks that belong to the critical path and schedules them with higher priority.

This paper includes a unique evaluation of performance, power and energy on a real AMC of real parallel applications.
This paper also reflects the impact of using different big and little core counts which is not present in previous works \cite{Cong_quickIA}.
All these works reflect the remarkable research that is taking place on AMCs. 
However we consider that their experimental evaluation is limited for three main reasons:
i)~The evaluation is done through a simulator or emulation of an AMC 
\cite{Kumar_micro_2003, Morad_area_based, Balakrishnan:ISCA2005, 
	Koufaty_bias, VanCraeynest_fairness, VanCraeynest_PIE, Rodrigues_thread_scheduling, Hetero93, 
	Hetero95, Dup09, Suleman:APLOS2009, Joao:ASPLOS2012,Joao:ISCA2013};
ii)~The evaluated applications are either random task dependency graph generators or scientific 
kernels and micro-benchmarks \cite{Hetero93,HEFT,LDCP}.
iii)~Their evaluation does not focus on power and energy consumption 
\cite{Kumar:ISCA2004, 
	VanCraeynest_fairness, VanCraeynest_PIE, Hetero95, Chronaki:ICS2015}.
\fi
